#!/usr/bin/env python3
"""
Fast parallel comparison of baseline vs after allium site output.

Workflow:
  1. Before change: python3 allium/allium.py --out allium/www_baseline --apis all
  2. Make code changes
  3. After change:  python3 allium/allium.py --out allium/www_after --apis all
  4. Compare:       python3 compare_outputs.py

Optimized for ~21k file pairs:
  - ProcessPoolExecutor across all CPUs (3.8x measured speedup)
  - Early exit for byte-identical files (skip regex entirely)
  - Single combined regex instead of 8 separate passes
  - Binary files compared by size only

Exit codes:
  0 = no real content differences (timestamp-only changes are expected)
  1 = real content differences or file additions/removals detected
"""

import argparse
import difflib
import os
import re
import sys
import time
from concurrent.futures import ProcessPoolExecutor


# ---------------------------------------------------------------------------
# Constants
# ---------------------------------------------------------------------------

# File extensions treated as binary — compare by size only, never read as text
BINARY_EXTENSIONS = frozenset({
    '.png', '.jpg', '.jpeg', '.gif', '.ico', '.svg',
    '.woff', '.woff2', '.ttf', '.eot',
})

# Text files that should never contain volatile timestamps — any byte
# difference is a real content change, no normalization needed
DIRECT_COMPARE_EXTENSIONS = frozenset({
    '.css', '.js', '.json', '.xml', '.txt',
})

# Single compiled regex matching ALL volatile patterns in one pass.
# Order matters: longer/more-specific patterns first to avoid partial matches.
#
# Validated against real allium output (22,169 files). These patterns cover:
#   - skeleton.html footer: "on Fri, 27 Feb 2026 09:08:02 GMT" (RFC 2822)
#   - relay-list.html header: "Last updated: ..."
#   - relay-info.html: "Last fetch was at Fri, 27 Feb 2026 09:08:02 GMT."
#   - relay-info.html: uptime "3w 2d 5h 17m ago", "UP 97%"
#   - macros.html: "Validation data last updated: ..."
#   - collector: "fetched 2026-02-21 06:55:20"
#   - ISO datetimes: "2026-02-21 06:55:20"
VOLATILE_RE = re.compile(
    r'Validation data last updated: [^\n<]+'          # AROI validation timestamp
    r'|Last updated: [^<]+'                            # relay-list header timestamp
    r'|Last fetch was at [^\n<]+'                      # fetch timestamp (any format)
    r'|fetched [^\n<]+'                                # collector fetched timestamp (any format)
    r'|\d+ (?:second|minute|hour|day|week|month|year)s?\s*ago'  # "5 days ago"
    r'|\d+[ywdhms]\s*(?:\d+[ywdhms]\s*)*(?:ago)?(?![a-zA-Z])'  # "3w 2d 5h 17m ago" (not "1st")
    r'|UP \d+%'                                        # uptime percentage
    r'|(?:Mon|Tue|Wed|Thu|Fri|Sat|Sun), \d{1,2} (?:Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec) \d{4} \d{2}:\d{2}:\d{2} \w+'  # RFC 2822 date
    r'|\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}'           # ISO datetime
    r'|Generated by.*?</a>'                            # footer generation line
)


# ---------------------------------------------------------------------------
# Worker function — runs in subprocess via ProcessPoolExecutor
# ---------------------------------------------------------------------------

def classify_file(args):
    """
    Classify a single file pair as identical / timestamp_only / content_diff.

    Runs in a worker process. Receives and returns only picklable primitives
    to minimize IPC overhead.

    Performance: uses line-level selective normalization — only the ~5-20 lines
    that actually differ get regex treatment, not the full file (31x faster on
    typical 124KB relay pages: 0.5ms vs 13.8ms).

    Returns:
        (category, rel_path) where category is one of:
        'identical', 'timestamp_only', 'content_diff', 'error'
    """
    rel_path, baseline_path, after_path = args
    ext = os.path.splitext(rel_path)[1].lower()

    # Binary files: compare by size only (never read content)
    if ext in BINARY_EXTENSIONS:
        try:
            if os.path.getsize(baseline_path) == os.path.getsize(after_path):
                return ('identical', rel_path)
        except OSError:
            pass
        return ('content_diff', rel_path)

    # Read both files as text
    try:
        with open(baseline_path, 'r', errors='replace') as f:
            baseline_text = f.read()
        with open(after_path, 'r', errors='replace') as f:
            after_text = f.read()
    except Exception:
        return ('error', rel_path)

    # Fast path: raw content byte-identical — no regex needed
    if baseline_text == after_text:
        return ('identical', rel_path)

    # Non-HTML text files (css/js/json): any difference is real
    if ext in DIRECT_COMPARE_EXTENSIONS:
        return ('content_diff', rel_path)

    # HTML files: selective line-level normalization.
    # Only normalize lines that actually differ (typically ~5-20 out of ~2500).
    baseline_lines = baseline_text.splitlines()
    after_lines = after_text.splitlines()

    if len(baseline_lines) != len(after_lines):
        # Line count changed — fall back to full normalization
        if VOLATILE_RE.sub('', baseline_text) == VOLATILE_RE.sub('', after_text):
            return ('timestamp_only', rel_path)
        return ('content_diff', rel_path)

    # Same line count: only normalize the lines that differ
    for b_line, a_line in zip(baseline_lines, after_lines):
        if b_line != a_line:
            if VOLATILE_RE.sub('', b_line) != VOLATILE_RE.sub('', a_line):
                return ('content_diff', rel_path)

    return ('timestamp_only', rel_path)


# ---------------------------------------------------------------------------
# File inventory
# ---------------------------------------------------------------------------

def get_files(directory):
    """Walk directory and return {relative_path: absolute_path} mapping."""
    files = {}
    for root, _, filenames in os.walk(directory):
        for fname in filenames:
            full = os.path.join(root, fname)
            rel = os.path.relpath(full, directory)
            files[rel] = full
    return files


# ---------------------------------------------------------------------------
# Diff display
# ---------------------------------------------------------------------------

def show_diffs(diff_files, baseline_files, after_files, max_lines):
    """Show normalized unified diffs for files with real content changes."""
    print("=" * 60)
    print("CONTENT DIFFS (normalized — timestamps replaced with [VOLATILE])")
    print("=" * 60)

    for fp in diff_files:
        bp = baseline_files.get(fp)
        ap = after_files.get(fp)
        if not bp or not ap:
            continue

        try:
            with open(bp, 'r', errors='replace') as f:
                baseline_lines = f.readlines()
            with open(ap, 'r', errors='replace') as f:
                after_lines = f.readlines()
        except Exception as e:
            print(f"\n--- {fp} --- ERROR: {e}")
            continue

        # Normalize both sides so diff shows only real changes
        bl_norm = [VOLATILE_RE.sub('[VOLATILE]', line) for line in baseline_lines]
        al_norm = [VOLATILE_RE.sub('[VOLATILE]', line) for line in after_lines]

        diff = list(difflib.unified_diff(
            bl_norm, al_norm,
            fromfile=f'baseline/{fp}',
            tofile=f'after/{fp}',
            lineterm='',
        ))

        if diff:
            print(f"\n--- {fp} ---")
            for line in diff[:max_lines]:
                print(line)
            if len(diff) > max_lines:
                print(f"  ... ({len(diff) - max_lines} more diff lines)")


# ---------------------------------------------------------------------------
# Categorization
# ---------------------------------------------------------------------------

def categorize_diffs(diff_files):
    """Group diff files by page type for the summary breakdown."""
    categories = {}
    for fp in diff_files:
        # Match on first path component
        first = fp.split('/')[0] if '/' in fp else ''
        cat_map = {
            'relay': 'relay pages',
            'contact': 'contact pages',
            'as': 'AS pages',
            'country': 'country pages',
            'family': 'family pages',
            'flag': 'flag pages',
            'platform': 'platform pages',
            'first_seen': 'first_seen pages',
            'misc': 'misc pages',
            'static': 'static files',
        }
        cat = cat_map.get(first, 'root pages')
        categories.setdefault(cat, []).append(fp)
    return categories


# ---------------------------------------------------------------------------
# Report
# ---------------------------------------------------------------------------

def print_report(args, results, only_baseline, only_after, common,
                 baseline_files, after_files, t0, t1, t2):
    """Print categorized comparison report with optional unified diffs."""
    t3 = time.time()

    # Header
    print("=" * 60)
    print("ALLIUM OUTPUT COMPARISON REPORT")
    print("=" * 60)
    print(f"Baseline: {args.baseline}")
    print(f"After:    {args.after}")
    print(f"Workers:  {args.workers}")
    print()

    # File counts
    print(f"Common files:        {len(common):>6}")
    print(f"  Identical:         {len(results['identical']):>6}")
    print(f"  Timestamp only:    {len(results['timestamp_only']):>6}")
    print(f"  Content diffs:     {len(results['content_diff']):>6}")
    if results['error']:
        print(f"  Errors:            {len(results['error']):>6}")
    print(f"Only in baseline:    {len(only_baseline):>6}")
    print(f"Only in after:       {len(only_after):>6}")
    print()

    # Content diff breakdown by page type
    if results['content_diff']:
        categories = categorize_diffs(results['content_diff'])
        print("Content diff breakdown:")
        for cat, files in sorted(categories.items()):
            print(f"  {cat}: {len(files)}")
        print()

    # Files only in one directory
    if only_baseline:
        print(f"FILES ONLY IN BASELINE ({len(only_baseline)}):")
        for f in only_baseline[:20]:
            print(f"  - {f}")
        if len(only_baseline) > 20:
            print(f"  ... and {len(only_baseline) - 20} more")
        print()

    if only_after:
        print(f"FILES ONLY IN AFTER ({len(only_after)}):")
        for f in only_after[:20]:
            print(f"  - {f}")
        if len(only_after) > 20:
            print(f"  ... and {len(only_after) - 20} more")
        print()

    # Unified diffs for content changes (unless --quiet)
    if results['content_diff'] and not args.quiet:
        show_diffs(
            results['content_diff'][:args.max_diffs],
            baseline_files, after_files, args.diff_lines,
        )

    # Timing
    print()
    print("-" * 60)
    elapsed = time.time() - t0
    print(
        f"Inventory: {t1 - t0:.2f}s | "
        f"Classify: {t2 - t1:.2f}s | "
        f"Report: {time.time() - t3:.2f}s | "
        f"Total: {elapsed:.2f}s"
    )

    # Verdict
    if not results['content_diff'] and not only_baseline and not only_after:
        print("PASS - No real content differences found")
    else:
        total_issues = (
            len(results['content_diff'])
            + len(only_baseline)
            + len(only_after)
        )
        print(f"REVIEW NEEDED - {total_issues} file(s) with real differences")


# ---------------------------------------------------------------------------
# Main
# ---------------------------------------------------------------------------

def main():
    parser = argparse.ArgumentParser(
        description='Fast parallel comparison of baseline vs after allium output.',
        epilog=(
            'Workflow:\n'
            '  1. python3 allium/allium.py --out allium/www_baseline --apis all\n'
            '  2. (make code changes)\n'
            '  3. python3 allium/allium.py --out allium/www_after --apis all\n'
            '  4. python3 compare_outputs.py\n'
        ),
        formatter_class=argparse.RawDescriptionHelpFormatter,
    )
    parser.add_argument(
        '--baseline', default='allium/www_baseline',
        help='Baseline output directory (default: allium/www_baseline)',
    )
    parser.add_argument(
        '--after', default='allium/www_after',
        help='After-change output directory (default: allium/www_after)',
    )
    parser.add_argument(
        '--workers', type=int, default=max(1, os.cpu_count() or 4),
        help=f'Parallel workers (default: {max(1, os.cpu_count() or 4)} = all CPUs)',
    )
    parser.add_argument(
        '--diff-lines', type=int, default=50,
        help='Max unified diff lines to show per file (default: 50)',
    )
    parser.add_argument(
        '--max-diffs', type=int, default=50,
        help='Max files to show full diffs for (default: 50)',
    )
    parser.add_argument(
        '-q', '--quiet', action='store_true',
        help='Summary only — suppress unified diff output',
    )
    args = parser.parse_args()

    # Validate directories exist
    for label, path in [('Baseline', args.baseline), ('After', args.after)]:
        if not os.path.isdir(path):
            print(f"Error: {label} directory not found: {path}")
            print(f"Run allium first:  python3 allium/allium.py --out {path} --apis all")
            sys.exit(2)

    t0 = time.time()

    # Phase 1: Inventory — walk both dirs, build file lists
    baseline_files = get_files(args.baseline)
    after_files = get_files(args.after)

    only_baseline = sorted(set(baseline_files) - set(after_files))
    only_after = sorted(set(after_files) - set(baseline_files))
    common = sorted(set(baseline_files) & set(after_files))
    t1 = time.time()

    # Phase 2: Parallel classify — distribute file pairs across workers
    work_items = [
        (fp, baseline_files[fp], after_files[fp])
        for fp in common
    ]

    results = {
        'identical': [],
        'timestamp_only': [],
        'content_diff': [],
        'error': [],
    }

    with ProcessPoolExecutor(max_workers=args.workers) as pool:
        for category, rel_path in pool.map(
            classify_file, work_items, chunksize=200
        ):
            results[category].append(rel_path)
    t2 = time.time()

    # Phase 3: Report — summary, diffs, timing, verdict
    print_report(
        args, results, only_baseline, only_after, common,
        baseline_files, after_files, t0, t1, t2,
    )

    # Exit code: 0 = clean, 1 = real diffs, 2 = usage error (above)
    has_real_diffs = bool(
        results['content_diff'] or only_baseline or only_after
    )
    sys.exit(1 if has_real_diffs else 0)


if __name__ == '__main__':
    main()
