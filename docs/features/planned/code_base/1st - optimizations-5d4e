 ALLIUM CODEBASE IMPROVEMENT PLAN
Top 10 Priority Improvements (Ordered by Impact)
1. Break Down the Monolithic Relays Class (4,819 lines)
Impact: ðŸ”´ CRITICAL - Reduces complexity by 70%, improves maintainability exponentially

Current Issues:

Single relays.py file contains 4,819 lines (41% of entire codebase)
Violates Single Responsibility Principle massively
Handles: API fetching, data processing, template rendering, statistics, categorization, intelligence analysis, AROI calculations, and file I/O
Makes testing, debugging, and onboarding extremely difficult
Proposed Architecture:

# Current: Everything in one God class
class Relays:  # 4,819 lines doing EVERYTHING

# Proposed: Separation of Concerns
class RelayDataManager:     # 500 lines - Data access/storage
class RelayProcessor:       # 600 lines - Data transformation
class StatisticsEngine:     # 400 lines - Network statistics
class CategorizationEngine: # 500 lines - Sorting/grouping
class PageGenerator:        # 800 lines - Template rendering
class RelayFacade:          # 200 lines - Orchestrates above
Detailed Refactoring Steps:

Extract Data Management (Week 1): Move all data storage, relay filtering, and data access patterns to RelayDataManager
Extract Processing Logic (Week 2): Move all transformation logic (_trim_platform, _preprocess_template_data, etc.) to RelayProcessor
Extract Statistics (Week 3): Move _calculate_network_health_metrics, percentile calculations to StatisticsEngine
Extract Categorization (Week 4): Move _categorize, _generate_smart_context to CategorizationEngine
Extract Rendering (Week 5): Move all write_* methods to PageGenerator with dependency injection
Create Facade (Week 6): Implement RelayFacade to orchestrate the above with backward compatibility
Benefits:

Each class < 1,000 lines (manageable size)
Unit testing becomes feasible (currently nearly impossible)
Parallel development possible (multiple devs can work simultaneously)
Memory profiling easier (can profile individual components)
Bug isolation dramatically improved
2. Eliminate Circular Dependencies and Improve Module Structure
Impact: ðŸ”´ HIGH - Prevents import errors, enables true modularity

Current Issues:

85 import statements across 19 library files
Tight coupling between modules (coordinator â†’ workers â†’ relays â†’ aroileaders)
Utils modules importing from each other creating dependency chains
Makes refactoring risky and painful
Dependency Mapping (Current State):

relays.py (4819 lines)
â”œâ”€â”€ aroileaders.py (1211 lines)
â”‚   â”œâ”€â”€ uptime_utils.py (679 lines)
â”‚   â”œâ”€â”€ bandwidth_utils.py (455 lines)
â”‚   â””â”€â”€ country_utils.py (485 lines)
â”œâ”€â”€ intelligence_engine.py (685 lines)
â”‚   â””â”€â”€ statistical_utils.py (337 lines)
â”œâ”€â”€ coordinator.py (332 lines)
â”‚   â””â”€â”€ workers.py (488 lines)
â””â”€â”€ [10+ other utility modules]
Proposed Layered Architecture:

Layer 1: Core Domain Models (no dependencies)
â”œâ”€â”€ models/relay.py
â”œâ”€â”€ models/operator.py
â””â”€â”€ models/network.py

Layer 2: Data Access (depends on Layer 1 only)
â”œâ”€â”€ repositories/relay_repository.py
â””â”€â”€ repositories/cache_repository.py

Layer 3: Business Logic (depends on Layers 1-2)
â”œâ”€â”€ services/statistics_service.py
â”œâ”€â”€ services/intelligence_service.py
â””â”€â”€ services/aroi_service.py

Layer 4: Infrastructure (depends on Layers 1-3)
â”œâ”€â”€ api/onionoo_client.py
â””â”€â”€ rendering/template_engine.py

Layer 5: Application (depends on all layers)
â””â”€â”€ app/allium_application.py
Implementation Steps:

Create domain models with clear interfaces
Establish dependency injection pattern
Migrate code layer by layer (bottom-up)
Add interface contracts (Python protocols/ABCs)
Validate no upward dependencies with import-linter
Benefits:

Clear separation of concerns
Testability increases 10x
Refactoring becomes safe
New features easier to add without breaking existing code
3. Consolidate Duplicate Template Logic
Impact: ðŸŸ¡ MEDIUM-HIGH - Reduces template size by 40%, improves consistency

Current Issues:

5,965 lines of template code across 25 HTML files
Significant logic duplication in templates (especially contact.html, relay-info.html)
Inline calculations and conditionals scattered throughout
Makes changes error-prone (must update multiple places)
Duplication Examples Found:

{# Appears in 8+ templates with variations #}
{% if relay.get('bandwidth') %}
    {{ format_bandwidth(relay['bandwidth']) }}
{% else %}
    N/A
{% endif %}

{# Country flag rendering duplicated 12+ times #}
<img src="{{ path_prefix }}static/images/cc/{{ country }}.png" 
     title="{{ country_name }}" ...>
Consolidation Strategy:

Create Comprehensive Macro Library (macros_v2.html):

render_bandwidth_cell(relay, field_name)
render_country_flags(countries, show_names=True)
render_uptime_badge(uptime_pct, context='relay'|'contact')
render_performance_indicator(value, percentile, thresholds)
render_intelligence_section(intelligence_data)
Extract Template Components:

components/
â”œâ”€â”€ relay_card.html
â”œâ”€â”€ operator_intelligence.html
â”œâ”€â”€ reliability_metrics.html
â”œâ”€â”€ network_summary.html
â””â”€â”€ performance_insights.html
Move Complex Logic to Python:

All percentage calculations â†’ pre-compute in Python
All conditional styling â†’ pre-compute CSS classes in Python
All data transformations â†’ use Jinja filters backed by Python
Implementation Priority:

Week 1: Extract macros from contact.html (most complex)
Week 2: Apply macros to relay-info.html
Week 3: Standardize across all listing pages
Week 4: Create component library
Expected Reduction:

5,965 lines â†’ ~3,500 lines (40% reduction)
DRY violations: 50+ instances â†’ 0
Template rendering performance: +15-20%
4. Implement Proper Configuration Management
Impact: ðŸŸ¡ MEDIUM - Eliminates hardcoded values, enables environment-specific configs

Current Issues:

Configuration scattered across multiple files (allium.py, coordinator.py, workers.py)
Hardcoded URLs, timeouts, thresholds throughout codebase
No environment-specific configuration (dev vs prod)
Magic numbers everywhere (30 data points minimum, 999 uptime scale, etc.)
Current Anti-patterns:

# In workers.py
api_response = urllib.request.urlopen(conn, timeout=30)  # Why 30?

# In uptime_utils.py
if count < 30:  # Why 30? No explanation
    return 0.0

# In aroileaders.py  
if underutilized_count >= 25:  # Magic number threshold
Proposed Configuration System:

# config/settings.py
from dataclasses import dataclass
from typing import Optional
import os

@dataclass
class APIConfig:
    onionoo_details_url: str = "https://onionoo.torproject.org/details"
    onionoo_uptime_url: str = "https://onionoo.torproject.org/uptime"
    onionoo_bandwidth_url: str = "https://onionoo.torproject.org/bandwidth"
    aroi_url: str = "https://aroivalidator.1aeo.com/latest.json"
    request_timeout: int = 30
    cache_hours: int = 12
    retry_attempts: int = 3
    retry_backoff: float = 2.0

@dataclass
class ProcessingConfig:
    min_uptime_datapoints: int = 30
    uptime_normalization_max: int = 999
    reliability_min_relays: int = 25
    underutilized_bandwidth_threshold: int = 10_000_000
    filter_downtime_days: int = 7

@dataclass
class OutputConfig:
    output_dir: str = "./www"
    bandwidth_units: str = "bits"  # "bits" or "bytes"
    show_progress: bool = False

class Settings:
    def __init__(self):
        self.api = APIConfig()
        self.processing = ProcessingConfig()
        self.output = OutputConfig()
    
    @classmethod
    def from_env(cls):
        """Load from environment variables"""
        settings = cls()
        settings.api.onionoo_details_url = os.getenv(
            'ONIONOO_DETAILS_URL', 
            settings.api.onionoo_details_url
        )
        # ... more environment overrides
        return settings
    
    @classmethod
    def from_file(cls, path: str):
        """Load from YAML/TOML config file"""
        # Implementation here
        pass
Implementation Plan:

Create centralized Settings class
Document all configuration options
Replace all hardcoded values with settings references
Add environment variable support
Create config file templates (dev.yaml, prod.yaml)
Benefits:

Single source of truth for configuration
Easy to understand what can be configured
Environment-specific deployments simple
Testing easier (can inject test configs)
Documentation auto-generated from dataclasses
5. Add Comprehensive Input Validation
Impact: ðŸ”´ HIGH - Critical security improvement, prevents crashes

Current Issues:

Minimal validation of Onionoo API responses
Assumes data structure is always correct
No schema validation
Can crash on malformed data
Type hints exist but not enforced at runtime
Security Risks Identified:

# Current code trusts API data blindly
relay = self.json["relays"][idx]  # No validation that idx is valid
contact = relay.get("contact", "")  # No validation of content
bandwidth = relay.get("observed_bandwidth", 0)  # Assumes numeric
Proposed Validation Strategy:

# Use Pydantic for runtime validation
from pydantic import BaseModel, Field, validator, AnyUrl
from typing import List, Optional
from datetime import datetime

class OnionooRelay(BaseModel):
    """Validated relay model with type checking"""
    fingerprint: str = Field(..., regex=r'^[A-F0-9]{40}$')
    nickname: str = Field(..., min_length=1, max_length=19)
    or_addresses: List[str] = Field(default_factory=list)
    running: bool
    flags: List[str] = Field(default_factory=list)
    observed_bandwidth: int = Field(ge=0)
    consensus_weight: int = Field(ge=0)
    contact: Optional[str] = Field(None, max_length=10000)
    platform: Optional[str] = None
    version: Optional[str] = None
    first_seen: datetime
    last_seen: datetime
    country: Optional[str] = Field(None, regex=r'^[a-z]{2}$')
    
    @validator('contact')
    def sanitize_contact(cls, v):
        """Sanitize contact to prevent injection"""
        if v:
            # Remove potentially dangerous characters
            return ''.join(c for c in v if c.isprintable())
        return v
    
    @validator('or_addresses')
    def validate_addresses(cls, v):
        """Validate IP addresses"""
        import ipaddress
        validated = []
        for addr in v:
            try:
                # Parse and validate each address
                ip_part = addr.split(':')[0].strip('[]')
                ipaddress.ip_address(ip_part)
                validated.append(addr)
            except ValueError:
                continue  # Skip invalid addresses
        return validated

class OnionooDetailsResponse(BaseModel):
    """Validated response from Onionoo details API"""
    version: str
    relays_published: datetime
    relays: List[OnionooRelay]
    
    @validator('relays')
    def check_relays_not_empty(cls, v):
        if not v:
            raise ValueError("Relays list cannot be empty")
        return v
Implementation Steps:

Define Pydantic models for all API responses
Add validation at API boundaries (workers.py)
Add validation decorators for critical functions
Implement graceful degradation (log invalid data, continue processing)
Add validation tests
Benefits:

Runtime type safety
Clear documentation of expected data structures
Prevents crashes from malformed data
Security improvement (validates untrusted input)
Better error messages
6. Reduce Template Complexity and Pre-compute More
Impact: ðŸŸ¡ MEDIUM - Improves performance by 20-30%, reduces template errors

Current Issues:

Templates still contain complex logic despite optimization efforts
Conditional formatting computed at render time
Multiple nested loops in templates
Performance bottlenecks during page generation
Template Complexity Examples:

{# From contact.html - 100+ lines of inline logic #}
{% set overall_pct = intelligence.performance_operator_overall_pct|int %}
{% set overall_diff = ((intelligence.performance_operator_overall_ratio|float - 
                        intelligence.performance_network_overall_ratio|float) / 
                        intelligence.performance_network_overall_ratio|float * 100)|round(0)|int %}
{% if overall_pct <= 10 %}
    <span style="color: #c82333; font-weight: bold;">
{% elif overall_pct >= 85 %}
    <span style="color: #2e7d2e; font-weight: bold;">
{% elif overall_pct < 50 %}
    <span style="color: #cc9900; font-weight: bold;">
{% else %}
    <span>
{% endif %}
Pre-computation Strategy:

# In relays.py - compute display-ready data structures
class ContactDisplayData:
    """Pre-computed display data for contact pages"""
    def __init__(self, contact_data, intelligence, network_stats):
        # Pre-compute everything templates need
        self.performance_indicator = self._compute_performance_indicator(
            intelligence.performance_operator_overall_pct
        )
        self.performance_diff_formatted = self._format_performance_diff(
            intelligence.performance_operator_overall_ratio,
            intelligence.performance_network_overall_ratio
        )
        # ... all other template logic
    
    def _compute_performance_indicator(self, percentile):
        """Convert percentile to display class and color"""
        if percentile <= 10:
            return {
                'css_class': 'performance-critical',
                'color': '#c82333',
                'label': 'Needs Improvement',
                'icon': 'âš ï¸'
            }
        elif percentile >= 85:
            return {
                'css_class': 'performance-excellent',
                'color': '#2e7d2e',
                'label': 'Excellent',
                'icon': 'âœ…'
            }
        # ... etc
Template Becomes Simple:

{# Simplified template - just display pre-computed data #}
<span class="{{ performance.css_class }}" 
      style="color: {{ performance.color }}">
    {{ performance.icon }} {{ performance.label }}
</span>
Target Goals:

No calculations in templates
No complex conditionals in templates
All styling decisions made in Python
Templates purely presentational
Benefits:

20-30% faster rendering
Easier to test (test Python code, not templates)
Easier to change styling (modify Python, not hunt through templates)
Reduced template errors
7. Implement Proper Error Handling and Logging
Impact: ðŸŸ¡ MEDIUM - Improves debugging, production reliability

Current Issues:

Inconsistent error handling (some places try/except, some don't)
Limited logging (mostly progress messages)
No structured logging
Difficult to debug production issues
No error recovery strategies
Current Error Handling Gaps:

# In workers.py - handles HTTP errors but not others
@handle_http_errors(...)
def fetch_onionoo_details(...):
    api_response = urllib.request.urlopen(conn, timeout=30).read()
    data = json.loads(api_response.decode("utf-8"))  # Can fail!
    # No validation of data structure
    # No handling of partial data
    return data

# In relays.py - silent failures
for relay in self.json["relays"]:
    if relay.get("platform"):
        # What if platform is malformed?
        parts = relay["platform"].split(" on ", 1)  # Can crash
Proposed Logging Strategy:

import logging
import structlog
from typing import Optional, Dict, Any

# Configure structured logging
structlog.configure(
    processors=[
        structlog.stdlib.add_log_level,
        structlog.stdlib.add_logger_name,
        structlog.processors.TimeStamper(fmt="iso"),
        structlog.processors.StackInfoRenderer(),
        structlog.processors.format_exc_info,
        structlog.processors.JSONRenderer()
    ],
    logger_factory=structlog.stdlib.LoggerFactory(),
)

class AlliumLogger:
    """Structured logger for Allium"""
    
    def __init__(self, name: str):
        self.logger = structlog.get_logger(name)
    
    def info(self, event: str, **kwargs):
        """Log info with context"""
        self.logger.info(event, **kwargs)
    
    def error(self, event: str, exc_info: bool = True, **kwargs):
        """Log error with full context"""
        self.logger.error(event, exc_info=exc_info, **kwargs)
    
    def api_call(self, api_name: str, url: str, status: str, **kwargs):
        """Structured API call logging"""
        self.logger.info(
            "api_call",
            api=api_name,
            url=url,
            status=status,
            **kwargs
        )

# Usage
logger = AlliumLogger(__name__)

try:
    data = fetch_onionoo_details(url)
    logger.api_call(
        api_name="onionoo_details",
        url=url,
        status="success",
        relay_count=len(data.get('relays', [])),
        response_size_kb=len(str(data)) // 1024
    )
except Exception as e:
    logger.error(
        "api_call_failed",
        api="onionoo_details",
        url=url,
        error_type=type(e).__name__,
        exc_info=True
    )
Error Recovery Strategies:

from tenacity import retry, stop_after_attempt, wait_exponential

@retry(
    stop=stop_after_attempt(3),
    wait=wait_exponential(multiplier=1, min=2, max=10)
)
def fetch_with_retry(url: str) -> Dict[str, Any]:
    """Fetch with exponential backoff retry"""
    return fetch_onionoo_details(url)

class GracefulDegradation:
    """Handle partial failures gracefully"""
    
    @staticmethod
    def process_relays_safe(relays: List[Dict]) -> List[Dict]:
        """Process relays, skip malformed ones"""
        valid_relays = []
        errors = []
        
        for idx, relay in enumerate(relays):
            try:
                # Validate and process
                validated_relay = OnionooRelay(**relay)
                valid_relays.append(validated_relay.dict())
            except Exception as e:
                errors.append({
                    'index': idx,
                    'fingerprint': relay.get('fingerprint', 'unknown'),
                    'error': str(e)
                })
        
        if errors:
            logger.warning(
                "relay_processing_errors",
                error_count=len(errors),
                valid_count=len(valid_relays),
                errors=errors[:5]  # Log first 5
            )
        
        return valid_relays
Implementation Steps:

Add structlog dependency
Create AlliumLogger wrapper
Replace all print statements with structured logging
Add contextual logging at key points
Implement retry logic for network calls
Add graceful degradation for data processing
Benefits:

Production debugging 10x easier
Can track performance bottlenecks
Error patterns visible
Better user experience (graceful failures)
8. Improve Test Coverage and Organization
Impact: ðŸŸ¡ MEDIUM - Prevents regressions, enables confident refactoring

Current Issues:

26 test files but coverage unclear
Mix of unit, integration, and system tests
Some tests are very large and complex
No clear testing strategy
Test utilities scattered
Test Coverage Analysis:

# Current test files (26 total)
tests/
â”œâ”€â”€ test_unit_workers.py
â”œâ”€â”€ test_integration_full_workflow.py
â”œâ”€â”€ test_system_real_api.py
â”œâ”€â”€ test_authorities.py
â”œâ”€â”€ ...
â””â”€â”€ 22 more test files

# Coverage gaps (estimated):
- relays.py: ~30% covered
- aroileaders.py: ~40% covered
- intelligence_engine.py: ~50% covered
- workers.py: ~60% covered
- coordinator.py: ~70% covered
Proposed Test Structure:

tests/
â”œâ”€â”€ unit/                    # Fast, isolated tests
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ utils/
â”‚   â””â”€â”€ ...
â”œâ”€â”€ integration/             # Test component interactions
â”‚   â”œâ”€â”€ test_api_integration.py
â”‚   â”œâ”€â”€ test_data_pipeline.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ system/                  # End-to-end tests
â”‚   â””â”€â”€ test_full_generation.py
â”œâ”€â”€ performance/             # Performance benchmarks
â”‚   â””â”€â”€ test_benchmarks.py
â”œâ”€â”€ security/               # Security-specific tests
â”‚   â””â”€â”€ test_xss_protection.py
â”œâ”€â”€ fixtures/               # Shared test data
â”‚   â”œâ”€â”€ onionoo_responses/
â”‚   â”œâ”€â”€ relay_data/
â”‚   â””â”€â”€ ...
â””â”€â”€ conftest.py            # Pytest configuration
Testing Strategy:

# Unit tests should be fast and isolated
class TestRelayProcessor:
    """Test relay processing logic in isolation"""
    
    def test_trim_platform_linux(self):
        processor = RelayProcessor()
        relay = {"platform": "Tor 0.4.7.13 on Linux"}
        processor.trim_platform(relay)
        assert relay["platform"] == "Linux"
        assert relay["platform_raw"] == "Tor 0.4.7.13 on Linux"
    
    @pytest.mark.parametrize("platform,expected", [
        ("Tor 0.4.7.13 on Linux", "Linux"),
        ("Tor 0.4.8.0 on Windows", "Windows"),
        ("Tor 0.4.7.13 on FreeBSD", "FreeBSD"),
    ])
    def test_trim_platform_various(self, platform, expected):
        processor = RelayProcessor()
        relay = {"platform": platform}
        processor.trim_platform(relay)
        assert relay["platform"] == expected

# Integration tests verify components work together
class TestRelayDataPipeline:
    """Test data flows through pipeline correctly"""
    
    def test_relay_processing_pipeline(self, sample_onionoo_data):
        # Test that data flows: API â†’ Processing â†’ Statistics
        coordinator = Coordinator(...)
        relay_set = coordinator.get_relay_set()
        assert relay_set is not None
        assert len(relay_set.json['relays']) > 0
        assert 'sorted' in relay_set.json

# Performance tests prevent regressions
class TestPerformance:
    """Ensure performance doesn't degrade"""
    
    @pytest.mark.benchmark
    def test_relay_categorization_performance(self, benchmark, large_relay_set):
        def categorize():
            large_relay_set._categorize()
        
        result = benchmark(categorize)
        # Should complete in < 5 seconds for 10k relays
        assert result.stats.mean < 5.0
Coverage Targets:

Unit tests: 80%+ coverage
Integration tests: Cover all major workflows
System tests: Full end-to-end generation
Performance tests: Track key operations
Benefits:

Confident refactoring (tests catch regressions)
Clear test organization
Faster test execution (better organization)
Performance tracking prevents degradation
9. Security Hardening and Audit Trail
Impact: ðŸ”´ HIGH - Critical for production use, regulatory compliance

Current Security State:

âœ… XSS protection (autoescape enabled)
âœ… Basic input sanitization
âš ï¸ Limited input validation
âŒ No audit logging
âŒ No security headers on generated pages
âŒ No rate limiting on API calls
âŒ No secrets management
Security Improvements Needed:

1. Add Security Headers to Generated Pages:

# In template renderer
class SecurityHeaders:
    """Generate security headers for static pages"""
    
    @staticmethod
    def get_headers() -> Dict[str, str]:
        return {
            'Content-Security-Policy': (
                "default-src 'self'; "
                "script-src 'none'; "
                "style-src 'self' 'unsafe-inline'; "
                "img-src 'self' data:; "
                "font-src 'self'; "
                "frame-ancestors 'none'; "
                "base-uri 'self'"
            ),
            'X-Content-Type-Options': 'nosniff',
            'X-Frame-Options': 'DENY',
            'X-XSS-Protection': '1; mode=block',
            'Referrer-Policy': 'strict-origin-when-cross-origin'
        }
    
    @staticmethod
    def generate_meta_tags() -> str:
        """Generate meta tags for HTML"""
        headers = SecurityHeaders.get_headers()
        return '\n'.join(
            f'<meta http-equiv="{key}" content="{value}">'
            for key, value in headers.items()
        )
2. Add API Rate Limiting:

from time import time, sleep
from collections import defaultdict

class RateLimiter:
    """Prevent API abuse"""
    
    def __init__(self, max_requests: int = 10, window_seconds: int = 60):
        self.max_requests = max_requests
        self.window = window_seconds
        self.requests = defaultdict(list)
    
    def allow_request(self, api_name: str) -> bool:
        """Check if request is allowed"""
        now = time()
        # Clean old requests
        self.requests[api_name] = [
            req_time for req_time in self.requests[api_name]
            if now - req_time < self.window
        ]
        
        if len(self.requests[api_name]) >= self.max_requests:
            return False
        
        self.requests[api_name].append(now)
        return True
    
    def wait_if_needed(self, api_name: str):
        """Wait if rate limit exceeded"""
        while not self.allow_request(api_name):
            sleep(1)
3. Add Audit Logging:

class AuditLogger:
    """Log all security-relevant events"""
    
    def log_api_access(self, api: str, url: str, success: bool):
        """Log external API access"""
        logger.info(
            "api_access",
            api=api,
            url=url,
            success=success,
            timestamp=datetime.now(timezone.utc).isoformat()
        )
    
    def log_file_generation(self, path: str, relay_count: int):
        """Log file generation"""
        logger.info(
            "file_generated",
            path=path,
            relay_count=relay_count,
            timestamp=datetime.now(timezone.utc).isoformat()
        )
    
    def log_data_validation_failure(self, data_type: str, error: str):
        """Log validation failures"""
        logger.warning(
            "validation_failed",
            data_type=data_type,
            error=error,
            timestamp=datetime.now(timezone.utc).isoformat()
        )
4. Secrets Management:

import os
from typing import Optional

class SecretsManager:
    """Secure secrets management"""
    
    @staticmethod
    def get_api_key(name: str) -> Optional[str]:
        """Get API key from secure storage"""
        # Try environment variable first
        key = os.getenv(f'ALLIUM_{name.upper()}_KEY')
        if key:
            return key
        
        # Try secrets file (e.g., Docker secrets, Kubernetes secrets)
        secret_path = f'/run/secrets/{name.lower()}_key'
        if os.path.exists(secret_path):
            with open(secret_path) as f:
                return f.read().strip()
        
        return None
Implementation Priority:

Add security headers (immediate)
Implement audit logging (week 1)
Add rate limiting (week 1)
Set up secrets management (week 2)
Run security audit with updated tools (week 2)
Benefits:

Production-ready security
Compliance with security standards
Audit trail for debugging and compliance
Protection against API abuse
10. Documentation and Code Organization
Impact: ðŸŸ¡ MEDIUM - Dramatically improves maintainability and onboarding

Current Issues:

Documentation scattered across 99+ markdown files
No clear architecture documentation
Inline code comments insufficient
No developer guide for contributing
API documentation missing
Documentation Gaps:

Current:
- README.md (362 lines) - too long, overwhelming
- 99+ doc files in /docs - hard to navigate
- No architecture diagrams
- No API documentation
- No contribution workflow guide
Proposed Documentation Structure:

docs/
â”œâ”€â”€ README.md                       # Quick links, overview
â”œâ”€â”€ getting-started/
â”‚   â”œâ”€â”€ installation.md
â”‚   â”œâ”€â”€ quickstart.md
â”‚   â””â”€â”€ configuration.md
â”œâ”€â”€ architecture/
â”‚   â”œâ”€â”€ overview.md                 # High-level architecture
â”‚   â”œâ”€â”€ data-flow.md               # How data flows through system
â”‚   â”œâ”€â”€ module-structure.md        # Module organization
â”‚   â””â”€â”€ diagrams/                  # Architecture diagrams
â”‚       â”œâ”€â”€ system-overview.png
â”‚       â”œâ”€â”€ data-pipeline.png
â”‚       â””â”€â”€ class-structure.png
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ relays-api.md
â”‚   â”œâ”€â”€ statistics-api.md
â”‚   â””â”€â”€ aroi-api.md
â”œâ”€â”€ development/
â”‚   â”œâ”€â”€ setup.md
â”‚   â”œâ”€â”€ testing.md
â”‚   â”œâ”€â”€ contributing.md
â”‚   â”œâ”€â”€ code-style.md
â”‚   â””â”€â”€ release-process.md
â”œâ”€â”€ security/
â”‚   â”œâ”€â”€ security-model.md
â”‚   â”œâ”€â”€ threat-analysis.md
â”‚   â””â”€â”€ security-testing.md
â””â”€â”€ reference/
    â”œâ”€â”€ configuration-reference.md
    â”œâ”€â”€ api-reference.md
    â””â”€â”€ cli-reference.md
Code Organization Improvements:

# Add comprehensive module docstrings
"""
Module: relay_processor

Processes raw relay data from Onionoo API into structured format for analysis.

Key Classes:
- RelayProcessor: Main processing orchestrator
- RelayValidator: Validates relay data integrity
- RelayTransformer: Transforms data for different use cases

Usage:
    processor = RelayProcessor(config)
    processed_relays = processor.process(raw_data)

Performance:
- Processes 10,000 relays in ~2 seconds
- Memory usage: ~100MB for 10,000 relays

See Also:
- docs/architecture/data-pipeline.md
- docs/api/relay-processor-api.md
"""

# Add comprehensive function docstrings
def calculate_consensus_weight_fraction(
    relay: Dict[str, Any],
    network_total: int
) -> float:
    """
    Calculate relay's consensus weight as fraction of total network weight.
    
    Formula: relay_weight / network_total_weight
    
    Args:
        relay: Relay dictionary containing consensus_weight field
        network_total: Total consensus weight of entire network
    
    Returns:
        Fraction between 0.0 and 1.0 representing relay's network share
    
    Raises:
        ValueError: If network_total is 0 or negative
        KeyError: If relay lacks consensus_weight field
    
    Examples:
        >>> relay = {'consensus_weight': 1000}
        >>> calculate_consensus_weight_fraction(relay, 100000)
        0.01
    
    Performance:
        O(1) - constant time calculation
    
    See Also:
        - calculate_consensus_weight_percentage()
        - docs/architecture/consensus-weight.md
    """
    if network_total <= 0:
        raise ValueError("network_total must be positive")
    
    if 'consensus_weight' not in relay:
        raise KeyError("relay must have consensus_weight field")
    
    return relay['consensus_weight'] / network_total
Implementation Steps:

Create architecture diagrams (use mermaid.js)
Write high-level architecture guide
Document each major module
Add inline documentation to all public APIs
Create developer onboarding guide
Set up documentation site (MkDocs or Sphinx)
Benefits:

New developers can onboard in hours instead of days
Clear understanding of system architecture
Easier to maintain and extend
Better collaboration
ðŸ“Š Implementation Roadmap
Phase 1: Foundation (Weeks 1-4)
Implement configuration management (#4)
Add input validation system (#5)
Set up structured logging (#7)
Add security improvements (#9)
Phase 2: Refactoring (Weeks 5-10)
Break down Relays class (#1) - incremental
Fix circular dependencies (#2)
Consolidate template logic (#3)
Pre-compute more display data (#6)
Phase 3: Quality (Weeks 11-14)
Improve test coverage (#8)
Complete documentation (#10)
Performance optimization pass
Security audit
Phase 4: Validation (Weeks 15-16)
End-to-end testing
Performance benchmarking
Security penetration testing
User acceptance testing
ðŸŽ¯ Success Metrics
Code Quality:

âœ… No file > 1,000 lines
âœ… Test coverage > 80%
âœ… No circular dependencies
âœ… All security tests pass
Performance:

âœ… Full generation < 2 minutes (10k relays)
âœ… Template rendering 20% faster
âœ… Memory usage < 500MB peak
Maintainability:

âœ… New developer onboarding < 4 hours
âœ… Average PR review time < 1 hour
âœ… Bug fix time reduced 50%
This plan prioritizes high-impact improvements that will dramatically improve code quality, security, and maintainability while preserving all existing functionality.
